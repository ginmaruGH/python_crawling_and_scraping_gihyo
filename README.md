# Python クローリング & スクレイピング

[出版サイト](https://gihyo.jp/book/2019/978-4-297-10738-3)

## 目次

- 1. クローリング・スクレイピングとは何か
  - 1-1. 本書が取り扱う領域
  - 1-2. Wgetによるクローリング
  - 1-3. スクレイピングに役立つUnixコマンド
  - 1-4. gihyo.jpのスクレイピング
  - 1-5. まとめ
- 2. Pythonではじめるクローリング・スクレイピング
  - 2-1. Pythonを使うメリット
  - 2-2. Pythonのインストールと実行
  - 2-3. Pythonの基礎知識
  - 2-4. Webページを取得する
  - 2-5. Webページからデータを抜き出す
  - 2-6. データをファイルに保存する
  - 2-7. Pythonによるスクレイピングの流れ
  - 2-8. URLの基礎知識
  - 2-9. まとめ
- 3. ライブラリによる高度なクローリング・スクレイピング
  - 3-1. HTMLのスクレイピング
  - 3-2. XMLのスクレイピング
  - 3-3. データベースに保存する
  - 3-4. クローラーとURL
  - 3-5. Pythonによるクローラーの作成
  - 3-6. まとめ
- 4. 実用のためのメソッド
  - 4-1. クローラーの特性
  - 4-2. 収集したデータの利用に関する注意
  - 4-3. クロール先の負荷に関する注意
  - 4-4. 繰り返しの実行を前提とした設計
  - 4-5. まとめ
- 5. クローリング・スクレイピングの実践とデータの活用
  - 5-1. データセットの取得と活用
  - 5-2. APIによるデータの収集と活用
  - 5-3. 時系列データの収集と活用
  - 5-4. オープンデータの収集と活用
  - 5-5. Webページの自動操作
  - 5-6. JavaScriptを使ったページのスクレイピング
  - 5-7. 取得したデータの活用
  - 5-8. まとめ
- 6. フレームワーク Scrapy
  - 6-1. Scrapyの概要
  - 6-2. Spiderの作成と実行
  - 6-3. 実践的なクローリング
  - 6-4. 抜き出したデータの処理
  - 6-5. Scrapyの設定
  - 6-6. Scrapyの拡張
  - 6-7. クローリングによるデータの収集と活用
  - 6-8. 画像の収集と活用
  - 6-9. まとめ
- 7. クローラーの継続的な運用・管理
  - 7-1. クローラーをサーバーで実行する
  - 7-2. クローラーの定期的な実行
  - 7-3. クローリングとスクレイピングの分離
  - 7-4. クローリングの高速化・非同期化
  - 7-5. クラウドを活用する
  - 7-6. まとめ
- Appendix Vagrantによる開発環境の構築
  - A-1. VirtualBoxとVagrant
    - A-1-1. VirtualBoxとは
    - A-1-2. Vagrantとは
  - A-2 CPUの仮想化支援機能を有効にする
    - A-2-1. Windows 10の場合
    - A-2-2. Windows 7の場合
    - A-2-3. ファームウェアの設定で仮想化支援機能を有効にする
  - A-3. VirtualBoxのインストール
  - A-4. Vagrantのインストール
  - A-5. 仮想マシンを起動する
  - A-6. ゲストOSにSSH接続する
    - A-6-1. Tera TermでSSH接続する
  - A-7. Pythonのスクリプトファイルを実行する
  - A-8. Linuxの基本操作
  - A-9. Vagrantで仮想マシンを操作するコマンド
    - A-9-1. 仮想マシンを起動する（vagrant up）
    - A-9-2. 仮想マシンを終了・再起動する（vagrant halt/reload）
    - A-9-3. 仮想マシンを削除する（vagrant destroy）
    - A-9-4. 仮想マシンの状態を表示する（vagrant status）
    - A-9-5. 仮想マシンにSSH接続する（vagrant ssh）
    - A-9-6. 仮想マシンをエクスポートする（vagrant package）
